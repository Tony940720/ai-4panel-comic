import requests
from bs4 import BeautifulSoup
from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.text_rank import TextRankSummarizer
import jieba

def fetch_article(url):
    headers = {"User-Agent": "Mozilla/5.0"}
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.text, "html.parser")
    paragraphs = soup.find_all('p')
    text = '\n'.join([p.text.strip() for p in paragraphs if len(p.text.strip()) > 20])
    return text

def summarize_text(text, num_sentences=3):
    parser = PlaintextParser.from_string(text, Tokenizer("chinese"))
    summarizer = TextRankSummarizer()
    summarizer.stop_words = set()
    summary = summarizer(parser.document, num_sentences)
    return [str(sentence) for sentence in summary]

if __name__ == "__main__":
    url = input("è«‹è¼¸å…¥æ–°èç¶²å€ï¼š")
    article = fetch_article(url)
    print("\nã€åŸæ–‡å‰300å­—ã€‘\n", article[:300])
    summary = summarize_text(article)
    print("\nã€è‡ªå‹•å¤§ç¶±ã€‘")
    for s in summary:
        print("ğŸ”¹", s)